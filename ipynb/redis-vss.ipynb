{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Similarity Search with Redis\n",
    "### Redis as a Vector Database\n",
    "\n",
    "with Brian Sam-Bodden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The \"Unstructured Data\" Problem\n",
    "\n",
    "- The **balanced** of data has changed radically... \n",
    "- **~80%** of the data generated by organizations is **Unstructured**<sup>(IDC report, 2020)</sup>\n",
    "- This percentage is estimated to keep growing <sup>(with CAGR of 36.5% between 2020 and 2025)</sup>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## But what is \"Unstructured\" Data?\n",
    "\n",
    "- Data that does not conform to a **pre-defined** data model\n",
    "- Data that can not be easily **\"indexed\"** by a search engine\n",
    "- Data is typically **high-dimensional** and **semantically** rich\n",
    "- Examples include **images**, **videos**, **free-form text**, and **audio**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![data pyramid](./images/data-balance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dealing with Unstructured Data\n",
    "\n",
    "- Unstructured data must be **transformed**\n",
    "- To deal with the **high-dimensional** nature we extract **\"features\"**\n",
    "- Traditional extraction techniques included **labelling**, **tagging**, and **1-hot encoding** \n",
    "- The extracted features are commonly encoded as **vectors** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Manual Image Feature Extraction\n",
    "\n",
    "![manual image feature extraction](./images/image-manual-feature-extraction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Manual Text Feature Extraction\n",
    "\n",
    "![manual text feature extraction](./images/text-manual-feature-extraction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vectors\n",
    "\n",
    "- They are a **Numeric representation** of something in **N-dimensional** space\n",
    "- Can represent **anything**... entire documents, images, video, audio \n",
    "- Quantifies **features** or **characteristics** of the item\n",
    "- More importantly... they are **comparable**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Vectors\n",
    "\n",
    "- A Vector is a tuple of one or more **values** called **scalars**\n",
    "- Each **scalar** represents the measure of a **feature**\n",
    "- Different frameworks use different data types to represent them:\n",
    "  - In **Numpy** they are **Numpy Arrays** (`np.arrays`)\n",
    "  - In **TensorFlow** they are **Tensors** (`tf.Tensor`)\n",
    "  - In **PyTorch** they are also **Tensors** (`torch.tensor`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3 \"Bicycle Reviews\" Features as a Vector\n",
    "\n",
    "![represenation of a vector](./images/bicycle_vector.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## üß® Issues with Feature Engineering\n",
    "\n",
    "- **Time-consuming**: Might require domain knowledge and expertise.\n",
    "- **High dimensionality**: Can lead to a high-dimensional feature space.\n",
    "- **Lack of scalability**: Not easily scalable, more data **==** more people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Enter \"Vector Embeddings\"\n",
    "\n",
    "- **Machine Learning** / **Deep Learning** have leaped forward in last decade \n",
    "- ML models **outperform** humans in many tasks nowadays\n",
    "  - üî• **CV** (Computer Vision) models excel at detection/classification\n",
    "  - üî• **LLMs** (Large Language Models) have advanced exponentially\n",
    "- Today, most vectors are **generated** using pre-trained **ML Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Enter \"Vector Embeddings\"\n",
    "\n",
    "- ML models can **extract contextual meaning** from unstructured data\n",
    "- Reduce semantically-rich high-dimensional inputs and **\"flatten\"** them \n",
    "- Flatten representations retain the semantic information and make for ideal vectors\n",
    "- Once in vector form the world of **linear algebra** allows to operate on vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Vector Embeddings from a CV Model\n",
    "\n",
    "![vector embedding extraction](./images/embedding-extraction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Enter \"Vector Databases\"\n",
    "\n",
    "- Pure Vector Databases **efficiently store** Vectors (along with **metadata**)\n",
    "- Enable **searching** for vectors using **\"similarity\"** and **\"distance\"** metrics\n",
    "- Enable **hybrid searches** combining vectors and metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Redis as a Vector Database\n",
    "\n",
    "- Redis provides **Search Capabilities** for structured/semi-structured data\n",
    "- Redis supports `TEXT`, `NUMERIC`, `TAG`, `GEO` and `GEOSHAPE` fields\n",
    "- Redis introduces the **`VECTOR`** schema field type for vector support \n",
    "- **`VECTOR`** field allows **indexing**, and **querying** vectors in **Hashes** or **JSON**\n",
    "- Redis **in-memory** approach provides **fast** and **efficient** vector searches\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Redis as a Vector Database\n",
    "\n",
    "- Capabilities:\n",
    "  - **3** distance metrics: **Euclidean**, **Internal Product** and **Cosine**\n",
    "  - **2** indexing methods: **HNSW** and **Flat**\n",
    "  - **Hybrid queries** combined with `GEO`, `TAG`, `TEXT` or `NUMERIC`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## üõ†Ô∏è Demo\n",
    "### Adding Similarity Search to the **Redis Bike Company**\n",
    "\n",
    "![bikeshop](./images/bike_shop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Connecting to Redis Stack\n",
    "\n",
    "* **Redis Stack** instance running locally\n",
    "* Import `redis-py` client library\n",
    "* Create a **client connection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import redis\n",
    "client = redis.Redis(host = 'localhost', port=6379, decode_responses=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Use the `PING` command to check that Redis is up and running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "client.ping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Inspect the Bikes\n",
    "\n",
    "* Use the `JSON.GET` command to retrive the bike with key `redisbikeco:bike:rbc00067`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "bike067 = client.json().get('redisbikeco:bike:rbc00067')\n",
    "bike067"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generating Embeddings with ML\n",
    "\n",
    "![ML Models for embeddings](./images/target-model-embeddings-redis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where to find pre-trained models?\n",
    "\n",
    "![Model Zoos](./images/model-zoos.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sentence Transformers\n",
    "\n",
    "![SBERT](./images/sbert-net.png)\n",
    "\n",
    "- **SentenceTransformers** to **generate embeddings** for the bikes **descriptions** \n",
    "- **Sentence-BERT** (**SBERT**) produces **contextually rich** sentence embeddings\n",
    "- Embeddings provide **efficient sentence-level** semantic similarity\n",
    "- Improves tasks like **semantic search** and **text grouping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Selecting a suitable pre-trained Model\n",
    "\n",
    "- We must pick a **suitable model** for **generating embeddings**\n",
    "- We want to query for bicycles using **short queries** against the **longer** bicycle **descriptions**\n",
    "- This is referred to as **\"Asymmetric Semantic Search\"** \n",
    "- Used when **search query** and the **documents** being searched are of **different nature or structure**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Selecting a suitable pre-trained Model\n",
    "\n",
    "- For **asymmetric semantic search** suitable models include pre-trained **MS MARCO** Models\n",
    "- Optimized for understanding **real-world queries** and producing **relevant responses**\n",
    "- **Highest performing** MS MARCO model is **`msmarco-distilbert-base-v4`**\n",
    "  - which is tuned for **cosine-similarity** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedder = SentenceTransformer('msmarco-distilbert-base-v4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Extract the Bike's Description\n",
    "\n",
    "- Let's extract the `description` into the `sample_description` var:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sample_description = bike067['description']\n",
    "sample_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generating an Embedding Vector\n",
    "\n",
    "- To generate the vector embeddings, we use the `encode` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "embedding = embedder.encode(sample_description)\n",
    "VECTOR_DIMENSION = len(embedding)\n",
    "VECTOR_DIMENSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's take a peek at the first **5** elements of the generated vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(embedding.tolist()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generate Embeddings for the Bikes' Description\n",
    "\n",
    "* To vectorize all the descriptions in the database, we will first collect all the Redis keys for the bikes:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "keys = sorted(client.keys('redisbikeco:bike:*'))\n",
    "len(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(keys[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generate Embeddings for the Bikes' Description\n",
    "\n",
    "* With the keys in `keys` we can use the Redis `JSON.MGET` command to retrieve just the `description` field\n",
    "* We'll store all the descriptions in the `descriptions` variable\n",
    "* The `encode` method can take a List of text passages to encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "descriptions = client.json().mget(keys, '$.description')\n",
    "descriptions = [item for sublist in descriptions for item in sublist]\n",
    "embeddings = embedder.encode(descriptions).astype(np.float32).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Let's checked that we've generated the correct number of embedding vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Add the embeddings to the JSON documents\n",
    "\n",
    "- Now we can add the vectorized descriptions to the JSON documents in Redis\n",
    "- Use the `JSON.SET` command to insert a new field in each of the documents at `$.description_embeddings`\n",
    "- Use Redis' **pipeline** mode to minimize the round-trip times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = client.pipeline()\n",
    "\n",
    "for key, embedding in zip(keys, embeddings):\n",
    "    pipeline.json().set(key, '$.description_embeddings', embedding)\n",
    "\n",
    "pipeline.execute()\n",
    "print('Vector Embeddings Saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Inspect the Bikes' Documents\n",
    "\n",
    "- Let's inspect one of the vectorized bike documents using the `JSON.GET` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(json.dumps(client.json().get('redisbikeco:bike:rbc00001'), indent=2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Create Search Index for the Bikes Collection\n",
    "\n",
    "- To define the index we'll import the `IndexDefinition` and the `IndexType`\n",
    "- To define the schema fields we'll use the classes `TagField`, `TextField`, `NumericField`, and **`VectorField`**\n",
    "- We'll create an index named **`idx:bikes_vss`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from redis.commands.search.indexDefinition import IndexDefinition, IndexType\n",
    "from redis.commands.search.field import TagField, TextField, NumericField, VectorField\n",
    "from redis.commands.search.query import Query\n",
    "\n",
    "INDEX_NAME = 'idx:bikes_vss'\n",
    "DOC_PREFIX = 'redisbikeco:bike:'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Search Index Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    client.ft(INDEX_NAME).info()\n",
    "    print('Index already exists!')\n",
    "except:\n",
    "    schema = (\n",
    "        TextField('$.model', no_stem=True, as_name='model'),  \n",
    "        TextField('$.brand', no_stem=True, as_name='brand'),\n",
    "        NumericField('$.price', as_name='price'),\n",
    "        TagField('$.type', as_name='type'),\n",
    "        TextField('$.description', as_name='description'),\n",
    "        VectorField('$.description_embeddings', 'FLAT', {\n",
    "          'TYPE': 'FLOAT32',\n",
    "          'DIM': VECTOR_DIMENSION,\n",
    "          'DISTANCE_METRIC': 'COSINE',\n",
    "        },  as_name='vector'),\n",
    "    )\n",
    "\n",
    "    # index Definition\n",
    "    definition = IndexDefinition(prefix=[DOC_PREFIX], index_type=IndexType.JSON)\n",
    "\n",
    "    # create Index\n",
    "    client.ft(INDEX_NAME).create_index(fields=schema, definition=definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## `VECTOR` Schema Field Definition\n",
    "\n",
    "* **Indexing method**: `FLAT` **(brute-force indexing)** or `HNSW` **(Hierarchical Navigable Small World)**\n",
    "* **Vector Type**: `FLOAT32` or `FLOAT64`.\n",
    "* **Vector Dimension**: The length or dimension of our embeddings (`768`).\n",
    "* **Distance Metric**: `L2` **(Euclidean distance)**, `IP` **(Inner Product)**, or `COSINE` **(Cosine Similarity)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Check the state of the Index\n",
    "\n",
    "- `FT.CREATE` creates the index\n",
    "- The **indexing process** is automatically started in the **background**\n",
    "- In the blink of an eye, our JSON documents are indexed and ready to be searched\n",
    "- To corroborate that, we use the **`FT.INFO`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "info = client.ft(INDEX_NAME).info()\n",
    "\n",
    "num_docs = info['num_docs']\n",
    "indexing_failures = info['hash_indexing_failures']\n",
    "total_indexing_time = info['total_indexing_time']\n",
    "percent_indexed = float(info['percent_indexed']) * 100\n",
    "\n",
    "\n",
    "print(f\"{num_docs} docs ({percent_indexed}%) indexed w/ {indexing_failures} failures in {float(total_indexing_time):.2f} msecs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Structured Data Searches with Redis\n",
    "\n",
    "- Let's test the non-vector part of the index first:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Retrieve all bikes where the `brand` is `Peaknetic`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "query = (\n",
    "    Query('@brand:Peaknetic').return_fields('id', 'brand', 'model', 'price')\n",
    ")\n",
    "client.ft(INDEX_NAME).search(query).docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Find all `Peaknetic` bikes price less than or equal to `10000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "query = (\n",
    "    Query('@brand:Peaknetic @price:[0 10000]').return_fields('id', 'brand', 'model', 'price')\n",
    ")\n",
    "client.ft(INDEX_NAME).search(query).docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Semantic Queries\n",
    "\n",
    "- We want to query for bikes using short query prompts\n",
    "- Let's put our queries in a list so we can vectorize them and execute them in bulk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "queries = [\n",
    "    'Bike for small kids',\n",
    "    'Best Mountain bikes for kids',\n",
    "    'Cheap Mountain bike for kids',\n",
    "    'Female specific mountain bike',\n",
    "    'Road bike for beginners',\n",
    "    'Commuter bike for people over 60',\n",
    "    'Comfortable commuter bike',\n",
    "    'Good bike for college students',\n",
    "    'Mountain bike for beginners',\n",
    "    'Vintage bike',\n",
    "    'Comfortable city bike'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "encoded_queries = embedder.encode(queries)\n",
    "len(encoded_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Visualizing Embeddings\n",
    "\n",
    "- The image below was generated using **t-distributed stochastic neighbor embedding** (**t-SNE**) and a small subset of the embeddings\n",
    "- **t-SNE** is a dimensionality reduction techniques that maps the higher dimension embeddings to a 2 or 3-D space\n",
    "\n",
    "![TSNE Visualization](./images/embeddings-tsne.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Constructing a \"Pure KNN\" VSS Query\n",
    "\n",
    "- We'll start with a **K-nearest neighbors** (KNN) query \n",
    "- KNN goal is to find the **most similar** items to a given query item\n",
    "- KNN calculates the **distance** between the query vector and each vector in the database\n",
    "- Returns 'K' items with the **smallest** distances\n",
    "- These are considered to be the most similar items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Constructing a \"Pure KNN\" VSS Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "query = (\n",
    "    Query('(*)=>[KNN 3 @vector $query_vector AS vector_score]')\n",
    "     .sort_by('vector_score')\n",
    "     .return_fields('vector_score', 'id', 'brand', 'model', 'description')\n",
    "     .dialect(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The syntax for KNN queries is `(*)=>[vector_similarity_query>]` \n",
    "  - where the `(*)` (the `*` meaning all) is the filter query for the search engine.\n",
    "  - `$query_vector` represents the query parameter we'll use to pass the vectorized query prompt.\n",
    "  - results are filtered by `vector_score`\n",
    "  - Query returns the `vector_score`, the `id` of the matched documents, the `$.brand`, `$.model`, and `$.description`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ü©º Pretty-printing Query Results\n",
    "\n",
    "- We want to run the queries in bulk \n",
    "- Visualize the results in a nice table\n",
    "- We've added a utility function `create_query_table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def create_query_table(query, queries, encoded_queries, extra_params = {}):\n",
    "    results_list = []\n",
    "    for i, encoded_query in enumerate(encoded_queries):\n",
    "        result_docs = client.ft(INDEX_NAME).search(query, { 'query_vector': np.array(encoded_query, dtype=np.float32).tobytes() } | extra_params).docs\n",
    "        for doc in result_docs:\n",
    "            vector_score = round(1 - float(doc.vector_score), 2)\n",
    "            # this is cosine distance\n",
    "            # cosine distance = 1 ‚Äî cosine similarity\n",
    "            results_list.append({\n",
    "                'query': queries[i], \n",
    "                'score': vector_score, \n",
    "                'id': doc.id,\n",
    "                'brand': doc.brand,\n",
    "                'model': doc.model,\n",
    "                'description': doc.description\n",
    "            })\n",
    "\n",
    "    # Pretty-print the table\n",
    "    queries_table = pd.DataFrame(results_list)\n",
    "    queries_table.sort_values(by=['query', 'score'], ascending=[True, False], inplace=True)\n",
    "    queries_table['query'] = queries_table.groupby('query')['query'].transform(lambda x: [x.iloc[0]] + ['']*(len(x)-1))\n",
    "    queries_table['description'] = queries_table['description'].apply(lambda x: (x[:497] + '...') if len(x) > 500 else x)\n",
    "    html = queries_table.to_html(index=False, classes='striped_table')  \n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## üèÉüèæ‚Äç‚ôÄÔ∏èRunning the Query\n",
    "\n",
    "- With the Query prepared in `query`\n",
    "- and the query prompts in `queries` \n",
    "- and the encoded queries in `encoded_queries`\n",
    "- we can use the `create_query_table` function to generate a table of results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## üèÉüèæ‚Äç‚ôÄÔ∏èRunning the Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "create_query_table(query, queries, encoded_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Hybrid Queries\n",
    "\n",
    "- \"Pure KNN\" queries evaluate a query against the **whole space of vectors**\n",
    "- The larger the collection, the more **computationally expensive**\n",
    "- Unstructured data does not live in isolation\n",
    "- Rich search experiences must allow searching all data (structured and unstructured) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Hybrid Queries\n",
    "\n",
    "- For example, users might arrive at your search interface with a brand preference in mind\n",
    "- Redis can use this information to pre-filter the search space\n",
    "- In the hybrid query definition below:\n",
    "  - we pre-filter using the `brand` to consider only `Peaknetic` brand bikes \n",
    "  - before our primary filter query was `(*)`, AKA everything\n",
    "  - we narrow the search space using `(@brand:Peaknetic)` before the KNN query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "hybrid_query = (\n",
    "    Query('(@brand:Peaknetic)=>[KNN 3 @vector $query_vector AS vector_score]')\n",
    "     .sort_by('vector_score')\n",
    "     .return_fields('vector_score', 'id', 'brand', 'model', 'description')\n",
    "     .dialect(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## üèÉüèæ‚Äç‚ôÄÔ∏èRunning the Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "create_query_table(hybrid_query, queries, encoded_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Range Queries\n",
    "\n",
    "- Range queries retrieve items within a specific **distance** from a query vector\n",
    "- We consider **\"distance\"** to be the **measure of similarity** \n",
    "- The smaller the distance, the more similar the items\n",
    "- For example, to return the top `4` bikes within a `0.55` radius of query: \n",
    "\n",
    "```\n",
    "1Ô∏è‚É£ FT.SEARCH idx:bikes_vss \n",
    "2Ô∏è‚É£   @vector:[VECTOR_RANGE $range $query_vector]=>{$YIELD_DISTANCE_AS: vector_score} \n",
    "3Ô∏è‚É£   SORTBY vector_score ASC\n",
    "4Ô∏è‚É£   LIMIT 0 4 \n",
    "5Ô∏è‚É£   DIALECT 2 \n",
    "6Ô∏è‚É£   PARAMS 4 range 0.55 query_vector \"\\x9d|\\x99>bV#\\xbfm\\x86\\x8a\\xbd\\xa7~$?*....\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Range Queries\n",
    "\n",
    "- In Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "range_query = (\n",
    "    Query('@vector:[VECTOR_RANGE $range $query_vector]=>{$YIELD_DISTANCE_AS: vector_score}') \n",
    "    .sort_by('vector_score')\n",
    "    .return_fields('vector_score', 'id', 'brand', 'model', 'description')\n",
    "    .paging(0, 4)\n",
    "    .dialect(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## üèÉüèæ‚Äç‚ôÄÔ∏èRunning the Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "create_query_table(range_query, queries[:1], encoded_queries[:1], {'range': 0.55})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualizing High-dimensional vectors with dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment",
     "tags": [
      "hide-input"
     ]
    }
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe src=\"https://projector.tensorflow.org/\" width=\"1920\" height=\"540\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap\n",
    "\n",
    "- The tools and techniques to unlock the value in **Unstructured Data** have evolved greatly...\n",
    "- Redis **in-memory first** approach makes it a perfect fit for vector similarity searches\n",
    "- Redis natively supports vector searches over **Hashes** and **JSON**\n",
    "- Redis combines the power of searching over semi-structured and unstructured data\n",
    "  - with the performance you've come to expect from Redis \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## https://github.com/redis-developer/redis-bike-co"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learn more at Redis University\n",
    "\n",
    "## `https://university.redis.com`\n",
    "\n",
    "![Redis U](./images/redis_university.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Thank You!\n",
    "\n",
    "![Simon and BSB](./images/simon_and_bsb.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "rise": {}
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
